{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNoht3Z1EPjj712Y98LWLBA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/icarus3/4castr/blob/master/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "QZ0-nbZSb2RR",
        "outputId": "00634815-0ed5-4068-8630-f40356182e6e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensortrade'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-50190f37ee60>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFeed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexchanges\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExchange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExchangeOptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulated\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexecute_order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstruments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUSD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBTC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensortrade'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import tensortrade.env.default as default\n",
        "from tensortrade.feed.core import Stream, DataFeed\n",
        "from tensortrade.oms.exchanges import Exchange, ExchangeOptions\n",
        "from tensortrade.oms.services.execution.simulated import execute_order\n",
        "from tensortrade.oms.instruments import USD, BTC\n",
        "from tensortrade.oms.wallets import Wallet, Portfolio\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "from stable_baselines3 import A2C, PPO\n",
        "import torch, random, optuna\n",
        "from ray import tune\n",
        "import ray, pickle\n",
        "from ray.tune.search.optuna import OptunaSearch\n",
        "from ray.tune.search import ConcurrencyLimiter\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "import copy\n",
        "# from sb3_contrib import RecurrentPPO\n",
        "# from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "\n",
        "def ada_df():\n",
        "    df = pd.read_csv(\"data/ada_hourly.csv\")\n",
        "\n",
        "    df.dropna(inplace=True)\n",
        "    df.drop_duplicates(inplace=True)\n",
        "\n",
        "    columns_to_copy = ['date', 'open', 'high', 'low', 'close', 'volume', 'trend_macd_diff', 'momentum_rsi']\n",
        "\n",
        "    return df[columns_to_copy]\n",
        "\n",
        "\n",
        "def get_feed(df):\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    features = []\n",
        "    for c in df.columns[1:]:\n",
        "        s = Stream.source(list(df[c]), dtype=\"float\").rename(df[c].name)\n",
        "        features += [s]\n",
        "\n",
        "    cp = Stream.select(features, lambda s: s.name == \"close\")\n",
        "    p = Stream.select(features, lambda s: s.name == \"close\")\n",
        "\n",
        "    new_features = [\n",
        "        cp.log().diff().fillna(0).rename(\"lr\"),\n",
        "    ]\n",
        "\n",
        "    selected_feature_name = ['open', 'high', 'low', 'volume', 'trend_macd_diff', 'momentum_rsi']\n",
        "    selected_feature = df[selected_feature_name]\n",
        "\n",
        "    # Fit and transform the selected features\n",
        "    selected_feature_scaled_array = scaler.fit_transform(selected_feature)\n",
        "\n",
        "    # Convert scaled features ndarray back to DataFrame\n",
        "    selected_feature_scaled = pd.DataFrame(selected_feature_scaled_array, columns=selected_feature_name)\n",
        "\n",
        "    # for k in selected_feature_name:\n",
        "    #    new_features.append(Stream.source(list(selected_feature_scaled[k]), dtype=\"float\").rename(k))\n",
        "\n",
        "    feed = DataFeed(new_features)\n",
        "    feed.compile()\n",
        "\n",
        "    return feed, p\n",
        "\n",
        "\n",
        "def get_env(df, feed, p):\n",
        "    bitstamp = Exchange(\"bitstamp\", service=execute_order, options=ExchangeOptions(commission=0.003))(\n",
        "        Stream.source(list(df[\"close\"]), dtype=\"float\").rename(\"USD-BTC\")\n",
        "    )\n",
        "\n",
        "    cash = Wallet(bitstamp, 10000 * USD)\n",
        "    asset = Wallet(bitstamp, 0 * BTC)\n",
        "\n",
        "    portfolio = Portfolio(USD, [\n",
        "        cash,\n",
        "        asset\n",
        "    ])\n",
        "\n",
        "    reward_scheme = default.rewards.PBR(price=p)\n",
        "    action_scheme = default.actions.BSH(\n",
        "        cash=cash,\n",
        "        asset=asset\n",
        "    ).attach(reward_scheme)\n",
        "\n",
        "    renderer_feed = DataFeed([\n",
        "        Stream.source(list(df[\"date\"])).rename(\"date\"),\n",
        "        Stream.source(list(df[\"open\"]), dtype=\"float\").rename(\"open\"),\n",
        "        Stream.source(list(df[\"high\"]), dtype=\"float\").rename(\"high\"),\n",
        "        Stream.source(list(df[\"low\"]), dtype=\"float\").rename(\"low\"),\n",
        "        Stream.source(list(df[\"close\"]), dtype=\"float\").rename(\"close\"),\n",
        "        Stream.source(list(df[\"volume\"]), dtype=\"float\").rename(\"volume\")\n",
        "    ])\n",
        "\n",
        "    env = default.create(\n",
        "        portfolio=portfolio,\n",
        "        action_scheme=action_scheme,\n",
        "        reward_scheme=reward_scheme,\n",
        "        feed=feed,\n",
        "        renderer_feed=renderer_feed,\n",
        "        renderer=default.renderers.PlotlyTradingChart(save_format=\"html\"),\n",
        "        window_size=24,\n",
        "        max_allowed_loss=0.5\n",
        "    )\n",
        "    return env\n",
        "\n",
        "\n",
        "df_train = ada_df()\n",
        "#train_size = 0.9\n",
        "#train_end = int(len(df_train) * train_size)\n",
        "train_end = df_train\n",
        "raw_df = df_train#df_train[:train_end]\n",
        "\n",
        "def sample_steps_batch_size():\n",
        "    n_steps = tune.randint(64, 2048)\n",
        "    n_steps_int = n_steps\n",
        "    factors = [i for i in range(1, n_steps_int + 1) if n_steps_int % i == 0]\n",
        "    batch_size = tune.choice(factors)\n",
        "    return {\"n_steps\": n_steps, \"batch_size\": batch_size}\n",
        "\n",
        "def objective2_test(config):\n",
        "    feed, p = get_feed(raw_df)\n",
        "    env = get_env(raw_df, feed, p)\n",
        "    obs, _ = env.reset()\n",
        "\n",
        "    # Define and train the agent\n",
        "    model = PPO(\"MlpPolicy\",\n",
        "                env,\n",
        "                verbose=1,\n",
        "                seed=42,\n",
        "                n_steps=int(config[\"batch_size\"] * 2),\n",
        "                batch_size=config[\"batch_size\"],\n",
        "                n_epochs=config[\"n_epochs\"],\n",
        "                gae_lambda=config[\"gae_lambda\"],\n",
        "                clip_range=config[\"clip_range\"],\n",
        "                max_grad_norm=config[\"max_grad_norm\"],\n",
        "                learning_rate=config[\"learning_rate\"],\n",
        "                gamma=config[\"gamma\"],\n",
        "                ent_coef=config[\"ent_coef\"],\n",
        "                vf_coef=config[\"vf_coef\"])\n",
        "\n",
        "    model.load(\"sb3-exp.ppo\")\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    rewards_array = []\n",
        "    while not done:\n",
        "        action, _states = model.predict(obs, deterministic=True)\n",
        "        #print(action)\n",
        "        obs, rewards, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        if not done:\n",
        "            rewards_array.append(rewards)\n",
        "\n",
        "    print({\"mean_reward\": sum(rewards_array) / len(rewards_array)})\n",
        "    env.render()\n",
        "\n",
        "def objective2(config):\n",
        "    # Create environment\n",
        "\n",
        "    feed, p = get_feed(raw_df)\n",
        "    env = get_env(raw_df, feed, p)\n",
        "    obs, _ = env.reset()\n",
        "\n",
        "    def make_env():\n",
        "        return copy.deepcopy(env)\n",
        "\n",
        "    env = make_vec_env(make_env, n_envs=25)\n",
        "\n",
        "    # Define and train the agent\n",
        "    model = PPO(\"MlpPolicy\",\n",
        "                env,\n",
        "                verbose=1,\n",
        "                seed=42,\n",
        "                n_steps=int(config[\"batch_size\"] * 2),\n",
        "                batch_size=config[\"batch_size\"],\n",
        "                n_epochs=config[\"n_epochs\"],\n",
        "                gae_lambda=config[\"gae_lambda\"],\n",
        "                clip_range=config[\"clip_range\"],\n",
        "                max_grad_norm=config[\"max_grad_norm\"],\n",
        "                learning_rate=config[\"learning_rate\"],\n",
        "                gamma=config[\"gamma\"],\n",
        "                ent_coef=config[\"ent_coef\"],\n",
        "                vf_coef=config[\"vf_coef\"])\n",
        "\n",
        "    model.learn(total_timesteps=100000000)\n",
        "    model.save(\"sb3-exp.ppo\")\n",
        "\n",
        "params={'learning_rate': 0.006140282577637256, 'batch_size': 1024, 'n_epochs': 29, 'gamma': 0.9432934745064481, 'gae_lambda': 0.9475058611459596, 'ent_coef': 0.10282313764715967, 'vf_coef': 0.2803686116565862, 'clip_range': 0.12024175041027786, 'max_grad_norm': 0.15292983461339837}\n",
        "objective2(params)\n",
        "objective2_test(params)\n",
        "\"\"\"\n",
        "ray.init()\n",
        "\n",
        "search_space = {\n",
        "    \"learning_rate\": tune.uniform(1e-5, 1e-2),\n",
        "    \"batch_size\": tune.choice([64, 128, 256, 512, 1024, 2048]),\n",
        "    \"n_epochs\": tune.randint(5, 30),\n",
        "    \"gamma\": tune.uniform(0.94, 0.99),\n",
        "    \"gae_lambda\": tune.uniform(0.94, 0.99),\n",
        "    \"ent_coef\": tune.uniform(0.01, 0.3),\n",
        "    \"vf_coef\": tune.uniform(0.01, 0.3),\n",
        "    \"clip_range\": tune.uniform(0.01, 0.3),\n",
        "    \"max_grad_norm\": tune.uniform(0.1, 0.9),\n",
        "}\n",
        "\n",
        "algo = OptunaSearch()\n",
        "algo = ConcurrencyLimiter(algo, max_concurrent=30)\n",
        "num_samples = 200\n",
        "\n",
        "tuner = tune.Tuner(\n",
        "    objective2,\n",
        "    tune_config=tune.TuneConfig(\n",
        "        metric=\"mean_reward\",\n",
        "        mode=\"max\",\n",
        "        search_alg=algo,\n",
        "        num_samples=num_samples,\n",
        "    ),\n",
        "    param_space=search_space,\n",
        ")\n",
        "results = tuner.fit()\n",
        "print(results)\n",
        "\n",
        "with open(\"results_sb3_ppo_pbr_exp.pkl\", \"wb\") as f:\n",
        "    pickle.dump(results, f)\n",
        "\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "best_params = study.best_params\n",
        "print(\"Best hyperparameters:\", best_params)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "raw_df = ada_df()\n",
        "feed, p = get_feed(raw_df)\n",
        "env = get_env(raw_df, feed, p)\n",
        "\n",
        "obs, _ = env.reset()\n",
        "\n",
        "model = A2C(\"MlpPolicy\", env=env, verbose=1, seed=seed, learning_rate=0.00007, ent_coef=0.01)\n",
        "model.learn(total_timesteps=1000000)\n",
        "\n",
        "obs, _ = env.reset()\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n",
        "\n",
        "env.render()\n",
        "\"\"\"\n"
      ]
    }
  ]
}